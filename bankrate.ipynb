{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classify all data points in 2017 into validation set, which is 42 available weeks\n",
    "n_valid = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scaling(df):\n",
    "    ''' This function is to standardize all columns in the data frame\n",
    "    Args:\n",
    "    df: data frame with all x and y\n",
    "    \n",
    "    Returns:\n",
    "    scaled: the standardized data\n",
    "    scaler: the object that stores the scaling informaiton which will be used to \n",
    "    inverse the standardized values to original values\n",
    "    '''\n",
    "    # ensure all data is float\n",
    "    values = df.values\n",
    "    values = values.astype('float32')\n",
    "    # standardize features\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    return (scaled, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some of the codes below refer to this article: \n",
    "# https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n",
    "def add_lags(df, n_lag=1, n_y=4):\n",
    "    ''' This function is to add week lags to the features used to predict mortgage rate\n",
    "    Args:\n",
    "    df: data frame with all x and y\n",
    "    n_lag: degree of lags used on all predictors\n",
    "    n_y: number of dependent variables in the data frame\n",
    "    \n",
    "    Returns:\n",
    "    agg: a data frame with all lagged predictors added\n",
    "    '''\n",
    "    n_vars = df.shape[1]-n_y\n",
    "    feats = pd.DataFrame(df).iloc[:,:-n_y]\n",
    "    y = pd.DataFrame(df).iloc[:,-n_y:]\n",
    "    cols, names = list(), list()\n",
    "    # add lags to predictors (t-i, ... t-1)\n",
    "    for i in range(n_lag, -1, -1):\n",
    "        cols.append(feats.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # combine all lagged predictors and y\n",
    "    agg = pd.concat((pd.concat(cols, axis=1), y), axis=1)\n",
    "    agg.columns = names + [('mortdiff(t+%d)' % (i+1)) for i in range(n_y)]\n",
    "    # drop rows with NaN values\n",
    "    agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_test_splitter(reframed_data, n_weeks, n_features, n_valid):\n",
    "    ''' This function is to split the traing and validation set '''\n",
    "    values = reframed_data.values\n",
    "    n_train_weeks = values.shape[0]-n_valid\n",
    "    train_X = values[:n_train_weeks, :(n_weeks*n_features)]\n",
    "    train_y = values[:n_train_weeks, -4:]\n",
    "    test_X = values[n_train_weeks:, :(n_weeks*n_features)]\n",
    "    test_y = values[n_train_weeks:, -4:]\n",
    "    # reshape input to 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], n_weeks, n_features))\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_weeks, n_features))\n",
    "    return (train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_rnn(train_X, n_output, lr=0.01, decay=0.02):\n",
    "    ''' This function is to build the RNN neural network structure\n",
    "    Args:\n",
    "    n_output: the dimension of output of the RNN layer\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(GRU(n_output, activation='tanh', return_sequences=False, \n",
    "                  input_shape=(train_X.shape[1], train_X.shape[2]),\n",
    "                  kernel_regularizer=regularizers.l2(0.0005), recurrent_regularizer=regularizers.l2(0.0005)))\n",
    "    model.add(Dense(4))\n",
    "    adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=decay)\n",
    "    model.compile(loss='mae', optimizer=adam)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(model, input0, layer):\n",
    "    ''' This function is to get the embedding vector (the output from the RNN layer)'''\n",
    "    get_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[layer].output])\n",
    "    layer_output = get_layer_output([input0])[0]\n",
    "    return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def invert_y(mort, scaler, test_X, test_y, yhat, n_features, n_weeks):\n",
    "    ''' This function is to invert the standardized values to original mortgage rate values\n",
    "    Returns:\n",
    "    inv_y: the actual mortgage rate values for next 4 weeks\n",
    "    inv_yhat: the rnn forecast of the original mortgage rate values for next 4 weeks\n",
    "    nb: the naive bayes forecast of the original mortgage rate values for next 4 weeks\n",
    "    '''\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_weeks*n_features))\n",
    "    # invert the standardized forecasts\n",
    "    inv_yhat = concatenate((test_X[:, -n_features:], yhat), axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,-4:]\n",
    "    # invert the standardized actual Y\n",
    "    inv_y = concatenate((test_X[:, -n_features:], test_y), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,-4:]\n",
    "    # add the differenced value back to the original mortgage rate to\n",
    "    # create forecasts of actual mortgage rate values\n",
    "    for i in xrange(4):\n",
    "        if i == 0:\n",
    "            inv_y[:,i] += np.array(mort[len(mort)-n_valid-4:len(mort)-4])\n",
    "            inv_yhat[:,i] += np.array(mort[len(mort)-n_valid-4:len(mort)-4])\n",
    "        else:\n",
    "            inv_y[:,i] += inv_y[:,i-1]\n",
    "            inv_yhat[:,i] += inv_yhat[:,i-1]\n",
    "    # generate the Naive Bayes forecast\n",
    "    nb = np.tile(np.array(mort[len(mort)-n_valid-4:len(mort)-4]),(4,1)).transpose()\n",
    "    return inv_y, inv_yhat, nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_eval(y, yhat):\n",
    "    ''' This function is to evaluate the results on validation dataset\n",
    "    Returns:\n",
    "    mae: the average MAE over all validation data\n",
    "    mape: the average MAPE over all validation data\n",
    "    '''\n",
    "    # calculate MAE and MAPE\n",
    "    mae = np.mean(abs(y - yhat))\n",
    "    mape = np.mean(abs(y - yhat)/y)\n",
    "    print ('Test MAE: %.3f' % mae)\n",
    "    print ('Test MAPE: %.3f' % mape)\n",
    "    return mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process1(features, mort, n_feats, n_weeks, n_valid, n_output, lr=0.01, decay=0.02, epochs=50, batch_size=52):\n",
    "    ''' This function is to build the complete modeling process including:\n",
    "    scaling and building variables, split training and validation set,\n",
    "    build the rnn model, train and evaluation the model\n",
    "    Returns:\n",
    "    mae: the average mae over all validation data\n",
    "    mape: the average mape over all validation data    \n",
    "    '''\n",
    "    scaled, scaler = scaling(features)\n",
    "    reframed = add_lags(scaled, n_weeks-1)\n",
    "    train_X, train_y, test_X, test_y = train_test_splitter(reframed, n_weeks, n_feats, n_valid)\n",
    "    model = build_rnn(train_X, n_output, lr=lr, decay=decay)\n",
    "    fit = model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, validation_data=(test_X, test_y), verbose=0, shuffle=False)\n",
    "    yhat = model.predict(test_X)\n",
    "    inv_y, inv_yhat, nb = invert_y(mort, scaler, test_X, test_y, yhat, n_feats, n_weeks)\n",
    "    # to achieve the forecasts, just comment out this line and modify the return to inv_yhat\n",
    "    mae, mape = model_eval(inv_y, inv_yhat)\n",
    "    return mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process2(features, mort, n_feats, n_weeks, n_valid, n_embed, n_output, lr1=0.01, lr2=0.005, epochs=50, batch_size=52):\n",
    "    ''' This function is to build the complete modeling process with a sophisticated RNN structure\n",
    "    Returns:\n",
    "    mae: the average mae over all validation data\n",
    "    mape: the average mape over all validation data    \n",
    "    '''\n",
    "    scaled, scaler = scaling(features)\n",
    "    reframed = add_lags(scaled, n_weeks-1)\n",
    "    # create a copy of train & test X for scaler inversion\n",
    "    train_X, train_y, test_X, test_y = train_test_splitter(reframed, n_weeks, n_feats, n_valid)\n",
    "    train_X1, train_y1, test_X1, test_y1 = train_test_splitter(reframed, n_weeks, n_feats-1, n_valid)\n",
    "    model1 = build_rnn(train_X1, n_embed, lr=lr1)\n",
    "    fit1 = model1.fit(train_X1, train_y1, epochs=epochs, batch_size=batch_size, validation_data=(test_X1, test_y1), verbose=0, shuffle=False)\n",
    "    embed1 = get_embedding(model1, np.concatenate([train_X1,test_X1]), 0)\n",
    "    yhat1 = get_embedding(model1, np.concatenate([train_X1,test_X1]), 1)\n",
    "    res1 = np.concatenate([train_y1,test_y1]) - yhat1\n",
    "    feats2 = np.concatenate([embed1,features.iloc[(n_weeks-1):,-5:-4],res1],axis=1)\n",
    "    embed2 = add_lags(feats2, n_weeks-1)\n",
    "    train_X2, train_y2, test_X2, test_y2 = train_test_splitter(embed2, n_weeks, embed1.shape[1]+1, n_valid)\n",
    "    model2 = build_rnn(train_X2, n_output, lr=lr2)\n",
    "    fit2 = model2.fit(train_X2, train_y2, epochs=epochs, batch_size=batch_size, validation_data=(test_X2, test_y2), verbose=0, shuffle=False)\n",
    "    yhat2 = model2.predict(test_X2)+yhat1[-n_valid:,:]\n",
    "    inv_y, inv_yhat, nb = invert_y(mort, scaler, test_X, test_y, yhat2, n_feats, n_weeks)\n",
    "    # to achieve the forecasts, just comment out this line and modify the return to inv_yhat\n",
    "    mae, mape = model_eval(inv_y, inv_yhat)\n",
    "    return mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data files generated by the R code\n",
    "mort30 = pd.read_csv('mort30.csv', index_col=0)\n",
    "mort15 = pd.read_csv('mort15.csv', index_col=0)\n",
    "mort5 = pd.read_csv('mort5.csv', index_col=0)\n",
    "morts = pd.read_csv('morts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select the most effective predictors\n",
    "morts30 = mort30.loc[:,['T10YIE', 'VXTYN', 'DGS10', 'MORTGAGE30US', 'week1', 'week2', 'week3', 'week4']]\n",
    "morts15 = mort15.loc[:,['T10YIE', 'MORTGAGE30US', 'DGS10', 'week1', 'week2', 'week3', 'week4']]\n",
    "morts5 = mort5.loc[:,['VXTYN', 'DFII10', 'week1', 'week2', 'week3', 'week4']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Belows are illustrations of one-time training and validation process for the three models.\n",
    "\n",
    "The results would be different every time the function is called due to the random starts of the weights unless a seed is set.\n",
    "\n",
    "The result in the report is the average returned value given by repeating this process 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.054\n",
      "Test MAPE: 0.013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.054015737, 0.013486803)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process1(morts30, morts.MORTGAGE30US, morts30.shape[1]-4, n_weeks=6, n_valid=n_valid, n_output=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.050\n",
      "Test MAPE: 0.015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.049600221, 0.015209377)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process1(morts15, morts.MORTGAGE15US, morts15.shape[1]-4, n_weeks=8, n_valid=n_valid, n_output=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.044\n",
      "Test MAPE: 0.014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.043587178, 0.013687313)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process1(morts5, morts.MORTGAGE5US, morts5.shape[1]-4, n_weeks=8, n_valid=n_valid, n_output=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Belows are illustrations of the more complicated recurrent neural network structure inspired by the Uber article.\n",
    "\n",
    "We can see that the performance are even poorer than the single-layer RNN.\n",
    "\n",
    "The reason of this is explained in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.062\n",
      "Test MAPE: 0.015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.061614711, 0.01535517)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process2(morts30, morts.MORTGAGE30US, morts30.shape[1]-4, n_weeks=6, n_valid=n_valid, n_embed=12, n_output=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.053\n",
      "Test MAPE: 0.016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.052523654, 0.016053999)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morts15_2 = mort15.loc[:,['T10YIE', 'MORTGAGE30US', 'DGS10', 'MORTGAGE15US', 'week1', 'week2', 'week3', 'week4']]\n",
    "process2(morts15_2, morts.MORTGAGE15US, morts15_2.shape[1]-4, n_weeks=8, n_valid=n_valid, n_embed=12, n_output=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.044\n",
      "Test MAPE: 0.014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.043606881, 0.013706421)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morts5_2 = mort5.loc[:,['VXTYN', 'DFII10', 'MORTGAGE5US', 'week1', 'week2', 'week3', 'week4']]\n",
    "process2(morts5_2, morts.MORTGAGE5US, morts5_2.shape[1]-4, n_weeks=8, n_valid=n_valid, n_embed=10, n_output=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
